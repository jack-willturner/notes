{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom floating point formats in machine learning \n",
    "\n",
    "It seems like every few weeks someone is announcing a new floating point format. This (hopefully evolving) blog post is to help me keep track of them.\n",
    "\n",
    "First a quick recap of how floating point works.\n",
    "\n",
    "## How floating point works\n",
    "\n",
    "The standard layout for floating point is:\n",
    "1. A sign bit\n",
    "2. Some exponent bits\n",
    "3. Some significand bits\n",
    "\n",
    "Each floating point format defines a **base** and a **precision** such that: \n",
    "\n",
    "$$\n",
    "n = \\text{sign} \\times \\text{significand} \\times \\text{base}^{\\text{exponent}}\n",
    "$$\n",
    "\n",
    "Where precision specifies the number of significant figures in the significand. E.g. for precision 4, it has the form `d.ddd`, etc. \n",
    "\n",
    "If we are in base 10, with precision 2, the number 1 can be written:\n",
    "\n",
    "$$\n",
    "1 \\times 1.00 \\times 10^{-1}\n",
    "$$\n",
    "\n",
    "If you want more detail, I highly recommend [this visual explanation](https://fabiensanglard.net/floating_point_visually_explained/).\n",
    "\n",
    "The core thing to understand is that we want to trade-off **range** (number of exponent bits) for **precision** (number of significand bits). \n",
    "\n",
    "If we assume most of deep learning happens in the range 0-1, that would mean we probably only ever touch one exponent bit maximum and could dedicate the rest to higher precision (we'll check this assumption later). There is a cost to this though; the physical size of a hardware multiplier scales with the _square_ of the significand width [1].\n",
    "\n",
    "### FP16\n",
    "\n",
    "![](figs/fp16.pdf)\n",
    "- 1 sign bit\n",
    "- 5 exponent bits\n",
    "- 10 significand bits\n",
    "\n",
    "Range: 1e^-38 to 3e^38\n",
    "Precision at X:\n",
    "\n",
    "### FP32\n",
    "\n",
    "- 1 sign bit\n",
    "- 8 exponent bits\n",
    "- 23 significand bits\n",
    "\n",
    "Range: 5.96e^(-8) to 65504\n",
    "Precision at X:\n",
    "\n",
    "### [BrainFloat16](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)\n",
    "\n",
    "![](figs/BrainFloat16.pdf)\n",
    "- 1 sign bit\n",
    "- 8 exponent bits\n",
    "- 7 significand bits\n",
    "\n",
    "Range: 1e-38 to 3e^(38)\n",
    "Precision at X:\n",
    "\n",
    "> The physical size of a hardware multiplier scales with the _square_ of the mantissa width.\n",
    "> neural networks are far more sensitive to the size of the exponent than that of the mantissa\n",
    "\n",
    "### [Nvidia TensorFloat32](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)\n",
    "\n",
    "![](figs/TensorFloat32.pdf)\n",
    "- 1 sign bit\n",
    "- 8 exponent bits\n",
    "- 10 significand bits\n",
    "\n",
    "Range:  5.96e^(-8) to 65504\n",
    "Precision at X:\n",
    "\n",
    "19 bits?!\n",
    "\n",
    "> TF32 uses the same 10-bit mantissa as the half-precision (FP16) math, shown to have more than sufficient margin for the precision requirements of AI workloads. And TF32 adopts the same 8-bit exponent as FP32 so it can support the same numeric range.\n",
    "\n",
    "### [DALL-e](https://arxiv.org/abs/2102.12092)\n",
    "\n",
    "> \"Adam moments are stored in 1-6-9 for the running mean (1-bit for the sign, 6-bits for the exponent, and 9-bits for the significand), and 0-6-10 for the variance\"\n",
    "\n",
    "### [Graphcore AIFloat](https://docs.graphcore.ai/projects/ai-float-white-paper/en/latest/ai-float.html#the-ipu-ai-floattm-format)\n",
    "- confusing\n",
    "\n",
    "\n",
    "### [Tesla CFP8](???)\n",
    "Can't find any info\n",
    "\n",
    "\n",
    "## Further reading\n",
    "- [An Nvidia guide](http://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf)\n",
    "- [A visual explanation](https://fabiensanglard.net/floating_point_visually_explained/)\n",
    "- [Demystifying floating point](https://blog.demofox.org/2017/11/21/floating-point-precision/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Bibliography\n",
    "\n",
    "[1] https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c62112cb75a8dcea2a59d5d6c130b4ea78ad2cfe8279f1846225255a82a039a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
